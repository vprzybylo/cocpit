{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from PIL import Image as pil_img\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image():\n",
    "    def __init__(self, open_dir, filename):\n",
    "        self.filename = filename\n",
    "        self.image_og = cv2.cvtColor(cv2.imread(open_dir+self.filename, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "        #if self.image_og.shape[2] == 4 or  self.image_og.shape[2] == 3:\n",
    "        self.height_og, self.width_og, channel = self.image_og.shape\n",
    "\n",
    "    def resize_stretch(self, desired_size):\n",
    "        self.im = cv2.resize(self.image_og, (desired_size, desired_size), interpolation = cv2.INTER_AREA)\n",
    "        self.height, self.width, channel = self.im.shape\n",
    "\n",
    "    def find_contours(self):\n",
    "        self.gray = cv2.cvtColor(self.im, cv2.COLOR_BGR2GRAY)\n",
    "        self.thresh = cv2.threshold(self.gray, 50, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "        self.contours, hierarchy = cv2.findContours(self.thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        self.contours = sorted(self.contours, key=cv2.contourArea, reverse = True)\n",
    "#         plt.imshow(self.thresh)\n",
    "#         plt.show()\n",
    "        \n",
    "    def morph_contours(self):\n",
    "        kernel = np.ones((5,5), dtype='uint8')\n",
    "        image_close = cv2.morphologyEx(self.thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        self.contours, hierarchy = cv2.findContours(image_close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  \n",
    "        draw=cv2.drawContours(self.thresh, self.contours, -1, (0,0,255), 2)\n",
    "        draw = cv2.fillPoly(self.thresh, self.contours, color=(255,255,255))\n",
    "#         plt.imshow(draw)\n",
    "#         plt.show()\n",
    "\n",
    "        self.contours, hierarchy = cv2.findContours(draw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        #self.contours = sorted(contours, key=cv2.contourArea, reverse = True)\n",
    "        \n",
    "    def mask_background(self):\n",
    "        mask = np.zeros(self.im.shape[:2], dtype=\"uint8\")\n",
    "        draw = cv2.drawContours(mask, [self.largest_contour()], 0, (255,255,255), -1)\n",
    "        self.im = cv2.bitwise_and(self.im, self.im, mask=mask)\n",
    "#         plt.imshow(self.im)\n",
    "#         plt.show()\n",
    "    def cutoff(self):\n",
    "        #checking the percentage of the contour that touches the edge/border\n",
    "        locations = np.where(self.thresh != 0)\n",
    "        count = 0 #pixels touching border\n",
    "        for xl,yl in zip(locations[0], locations[1]):\n",
    "            if xl == 0 or yl == 0 or xl == self.height-1 or yl == self.width-1:\n",
    "                count+=1\n",
    "        cutoff_perc = (count/(2*self.height+2*self.width))*100\n",
    "        return cutoff_perc\n",
    "    \n",
    "    def contrast(self):\n",
    "        return self.im.std()\n",
    "        \n",
    "    def laplacian(self):\n",
    "        return cv2.Laplacian(self.gray,cv2.CV_64F).var()\n",
    "    \n",
    "    def edges(self):\n",
    "        min_threshold = 0.66 * np.mean(self.im)\n",
    "        max_threshold = 1.33 * np.mean(self.im)\n",
    "        edges = cv2.Canny(self.im, min_threshold, max_threshold)\n",
    "        return edges\n",
    "\n",
    "    def largest_contour(self):\n",
    "        return sorted(self.contours, key=cv2.contourArea, reverse = True)[0] \n",
    "       \n",
    "    def area(self):\n",
    "        return cv2.contourArea(self.largest_contour())  \n",
    "\n",
    "    def perim(self):\n",
    "        return cv2.arcLength(self.largest_contour(), False)\n",
    "    \n",
    "    def phi(self):\n",
    "        rect = cv2.minAreaRect(self.largest_contour()) #box ONLY around the largest contour \n",
    "        #get length and width of contour\n",
    "        x = rect[1][0]\n",
    "        y = rect[1][1]      \n",
    "        self.rect_length = max(x,y)\n",
    "        self.rect_width = min(x,y)\n",
    "        return self.rect_width/self.rect_length\n",
    "        \n",
    "    def extreme_points(self):\n",
    "        cnt = self.largest_contour()\n",
    "        leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])\n",
    "        rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])\n",
    "        topmost = tuple(cnt[cnt[:,:,1].argmin()][0])\n",
    "        bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])\n",
    "        return np.std([leftmost, rightmost, topmost, bottommost])\n",
    "   \n",
    "    def filled_circular_area_ratio(self):  #similar to solidity\n",
    "        (x,y), radius = cv2.minEnclosingCircle(self.largest_contour())\n",
    "        center = (int(x),int(y))\n",
    "        circle = cv2.circle(self.thresh, center, int(radius), (255,255,255), 5)\n",
    "        #print(self.area()/(np.pi*radius**2), self.area())\n",
    "#         plt.imshow(circle)\n",
    "#         plt.show()\n",
    "        return self.area()/(np.pi*radius**2)\n",
    "\n",
    "    def circularity(self):\n",
    "        return (4.*np.pi*self.area())/(self.perim()**2)\n",
    "    \n",
    "    def roundness(self):\n",
    "        return (4.*np.pi*self.area())/(self.convex_perim(True)**2)\n",
    "                                       \n",
    "    def perim_area_ratio(self):\n",
    "        return self.perim()/self.area()\n",
    "        \n",
    "    def convex_perim(self, closed_cnt):\n",
    "        hull = cv2.convexHull(self.largest_contour())\n",
    "        return cv2.arcLength(hull, closed_cnt)\n",
    "    \n",
    "    def convexity(self):\n",
    "        return self.convex_perim()/self.perim()\n",
    "    \n",
    "    def complexity(self):\n",
    "        return 10*(0.1-(self.area()/(np.sqrt(self.area()/self.hull_area())*self.perim()**2)))\n",
    "    \n",
    "    def solidity(self):\n",
    "        return float(self.area())/self.hull_area()\n",
    "    \n",
    "    def equiv_d(self):\n",
    "        return np.sqrt(4*self.area()/np.pi)\n",
    "    \n",
    "    def hull_area(self):\n",
    "        hull = cv2.convexHull(self.largest_contour())      \n",
    "        return cv2.contourArea(hull)\n",
    "        \n",
    "    def save_image(self, save_dir, flip = False):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        if flip:\n",
    "            self.flip_imgs(save_dir)\n",
    "        else:\n",
    "            #save single image, no flipping:\n",
    "            self.im =cv2.cvtColor(self.im, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(os.path.join(save_dir,str(self.filename)), np.array(self.im))\n",
    "        \n",
    "    def show_image(self):\n",
    "        plt.imshow(self.image_og)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #open_dirs = ['../cpi_data/training_datasets/SPHERES/good/', '../cpi_data/training_datasets/SPHERES/bad/']\n",
    "    open_dirs = ['../cpi_data/training_datasets/SPHERES/good/', '../cpi_data/training_datasets/SPHERES/bad/'] \n",
    "    desired_size = 1000\n",
    "    count=0\n",
    "        \n",
    "    #Independent Variable\n",
    "    good_bad = []\n",
    "    \n",
    "    #Dependent Variables\n",
    "    cutoff = []\n",
    "    contrast = [] \n",
    "    height = []\n",
    "    width = []\n",
    "    lapl = []\n",
    "    area=[]\n",
    "    perim = []\n",
    "    phi = []\n",
    "    extreme_points = []\n",
    "    filled_circular_area_ratio=[]\n",
    "    circularity = []\n",
    "    roundness = []\n",
    "    perim_area_ratio = []\n",
    "    convex_perim = []\n",
    "    complexity = []\n",
    "    solidity = []\n",
    "    equiv_d = []\n",
    "    hull_area = []\n",
    "    std = []\n",
    "    contours = []\n",
    "    edges = []\n",
    "    cnt_area = []\n",
    "    \n",
    "    for direct in open_dirs:\n",
    "        \n",
    "        for filename in os.listdir(direct):\n",
    "            #want a good/bad index for every file\n",
    "            if direct == open_dirs[0]:\n",
    "                good_bad.append(0)\n",
    "            else:\n",
    "                good_bad.append(1)\n",
    "                \n",
    "            \n",
    "            image = Image(direct, filename)\n",
    "            image.resize_stretch(desired_size)\n",
    "            image.find_contours()\n",
    "            if len(image.contours)!=0 and image.area() != 0.0:\n",
    "                image.morph_contours()\n",
    "                #image.mask_background()\n",
    "            \n",
    "                count_edge_px = np.count_nonzero(image.edges())\n",
    "                if count_edge_px > 0:\n",
    "                    std.append(np.std(np.nonzero(image.edges())))\n",
    "                else:\n",
    "                    std.append(0)\n",
    "                lapl.append(image.laplacian())\n",
    "                contours.append(len(image.contours))\n",
    "                edges.append(count_edge_px)\n",
    "                contrast.append(image.contrast())\n",
    "                height.append(image.height_og)\n",
    "                width.append(image.width_og)\n",
    "                cnt_area.append(image.area())\n",
    "                solidity.append(image.solidity())\n",
    "                complexity.append(image.complexity())\n",
    "                equiv_d.append(image.equiv_d())\n",
    "                convex_perim.append(image.convex_perim(True))\n",
    "                hull_area.append(image.hull_area())\n",
    "                perim.append(image.perim())\n",
    "                phi.append(image.phi())\n",
    "                circularity.append(image.circularity())\n",
    "                cutoff.append(image.cutoff())\n",
    "                perim_area_ratio.append(image.perim_area_ratio())\n",
    "                roundness.append(image.roundness())\n",
    "                filled_circular_area_ratio.append(image.filled_circular_area_ratio())\n",
    "                extreme_points.append(image.extreme_points())\n",
    "            else:\n",
    "                image.show_image()\n",
    "                lapl.append(0)\n",
    "                contours.append(0)\n",
    "                edges.append(0)\n",
    "                contrast.append(0)\n",
    "                height.append(0)\n",
    "                width.append(0)\n",
    "                cnt_area.append(0)\n",
    "                solidity.append(0)\n",
    "                complexity.append(0)\n",
    "                equiv_d.append(0)\n",
    "                convex_perim.append(0)\n",
    "                hull_area.append(0)\n",
    "                perim.append(0)\n",
    "                phi.append(0)\n",
    "                circularity.append(0)\n",
    "                cutoff.append(0)\n",
    "                perim_area_ratio.append(0)\n",
    "                roundness.append(0)\n",
    "                filled_circular_area_ratio.append(0)\n",
    "                extreme_points.append(0)\n",
    "                std.append(0)\n",
    "    \n",
    "    dicts = {}\n",
    "    keys = ['good_bad', 'height', 'width', 'lapl', 'contours', 'edges', 'std', 'cnt_area', \\\n",
    "           'contrast', 'circularity', 'solidity','complexity','equiv_d','convex_perim',\\\n",
    "           'hull_area', 'perim', 'phi', 'cutoff', 'extreme_points' ,\\\n",
    "            'filled_circular_area_ratio','roundness','perim_area_ratio']\n",
    "    values = [good_bad, height, width, lapl, contours, edges, std, cnt_area, \\\n",
    "           contrast, circularity, solidity, complexity, equiv_d, convex_perim,\\\n",
    "           hull_area, perim, phi, cutoff, extreme_points, filled_circular_area_ratio,\\\n",
    "              roundness, perim_area_ratio]\n",
    "    for key, val in zip(keys, values):\n",
    "        print(key, len(val))\n",
    "        dicts[key] = val\n",
    "\n",
    "    df = pd.DataFrame(dicts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "if __name__ == '__main__':\n",
    "    df = main()\n",
    "    #df_new = df.drop(df[(df['height'] == 1000) & (df['width'] == 1000)].index)\n",
    "    #df.to_pickle(\"../saved_models/SPHERES.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform SPHERES model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dataframe with image attributes for training \n",
    "df_SPHERES = pd.read_pickle(\"/data/data/saved_models/no_mask/spheres_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SPHERES.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into x,y\n",
    "x = df_SPHERES.drop('good_bad',axis=1)\n",
    "y = df_SPHERES['good_bad']\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictors:\n",
    "X_train.columns\n",
    "#heigth and width are from original image dimensions\n",
    "#laplacian=blurriness\n",
    "#contours=# of contours\n",
    "#edges=# of edges\n",
    "#std = standard deviation in edge locations\n",
    "#cnt_area = contour area for largest contour\n",
    "#extreme_points = standard deviation in most extreme edge or point locations (top, left, bottom, right)\n",
    "#the rest are different measures of roundness/circularity/etc. or self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform with minmax scaler\n",
    "#all attributes are scaled to a fixed range between 0 and 1\n",
    "scaler_SPHERES_norm = MinMaxScaler() \n",
    "X_norm_train = scaler_SPHERES_norm.fit_transform(X_train)\n",
    "X_norm_test = scaler_SPHERES_norm.transform(X_test)\n",
    "\n",
    "#transform with standard scalar\n",
    "#mean=0, std=1, normalization \n",
    "scaler_SPHERES_stand = StandardScaler() \n",
    "X_stand_train = scaler_SPHERES_stand.fit_transform(X_train)\n",
    "X_stand_test = scaler_SPHERES_stand.transform(X_test)\n",
    "\n",
    "#If the distribution of the quantity is normal, then it should be standardized, otherwise, the data should be normalized.\n",
    "#trying both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also include principal component analysis to speed predictions up \n",
    "#let's see how many components are needed to explain variance\n",
    "pca = PCA()\n",
    "#X_log_train = np.log(X_train)\n",
    "#X_log_test = np.log(X_test)\n",
    "X_train_pca = pca.fit_transform(X_stand_train)\n",
    "X_test_pca = pca.transform(X_stand_test)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.scatter(np.arange(21), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['height', 'width', 'blurriness', 'contours', 'edge count', 'edge location spread', 'contour area',\n",
    "       'contrast', 'circularity', 'solidity', 'complexity', 'equiv-diameter',\n",
    "       'convex perim', 'hull area', 'perimeter', 'aspect ratio', 'cutoff', 'extreme points',\n",
    "       'circle-area ratio', 'roundness', 'perim-area ratio']\n",
    " \n",
    "\n",
    "fig , ax = plt.subplots(figsize=(8,5))\n",
    "plt.title('Original Data')\n",
    "g = sns.boxplot(data=pd.DataFrame(X_train, columns = X_train.columns))\n",
    "plt.xticks(plt.xticks()[0], labels)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90);\n",
    "#fig.savefig('../plots/spheres_og_boxplot_dist.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fig , ax = plt.subplots(figsize=(8, 5))\n",
    "plt.title('Normalized Data')\n",
    "sns.boxplot(data=pd.DataFrame(X_norm_train, columns = X_train.columns))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90);\n",
    "plt.xticks(plt.xticks()[0], labels)\n",
    "#fig.savefig('../plots/spheres_norm_boxplot_dist.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "fig , ax = plt.subplots(figsize=(8, 5))\n",
    "plt.title('Standardized Data')\n",
    "sns.boxplot(data=pd.DataFrame(X_stand_train, columns = X_train.columns))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90);\n",
    "plt.xticks(plt.xticks()[0], labels)\n",
    "plt.ylim(-6, 11)\n",
    "#fig.savefig('../plots/spheres_stand_boxplot_dist.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "fig , ax = plt.subplots(figsize=(8, 5))\n",
    "plt.title('PCA')\n",
    "sns.boxplot(data=pd.DataFrame(X_train_pca, columns = X_train.columns))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90);\n",
    "plt.xticks(plt.xticks()[0], labels)\n",
    "plt.ylim(-6, 11)\n",
    "#fig.savefig('../plots/spheres_pca_boxplot_dist.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#define binary logistic regression model:\n",
    "#balance uneven classes in training set for spheres/not spheres (good/bad)\n",
    "#set random state for reproduciblity and consistency between runs\n",
    "lg1 = LogisticRegression(random_state=13, class_weight='balanced')  \n",
    "\n",
    "#standardized transformations:\n",
    "lg1.fit(X_stand_train,y_train)  #don't normalize/standardize y since already between 0-1 for categorical\n",
    "y_pred = lg1.predict(X_stand_test)\n",
    "\n",
    "# performance\n",
    "print(f'Accuracy Score: {accuracy_score(y_test,y_pred)}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#now try with normalized transformation\n",
    "# define model\n",
    "lg1 = LogisticRegression(random_state=13, class_weight='balanced')\n",
    "# fit it\n",
    "lg1.fit(X_norm_train,y_train)  #don't normalize/standardize y since already between 0-1 for categorical \n",
    "# test\n",
    "y_pred = lg1.predict(X_norm_test)\n",
    "# performance\n",
    "print(f'Accuracy Score: {accuracy_score(y_test,y_pred)}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#and lastly with pca \n",
    "# define model\n",
    "lg1 = LogisticRegression(random_state=13, class_weight='balanced')\n",
    "# fit it\n",
    "lg1.fit(X_train_pca,y_train)  #don't normalize/standardize y since already between 0-1 for categorical \n",
    "# test\n",
    "y_pred = lg1.predict(X_test_pca)\n",
    "# performance\n",
    "print(f'Accuracy Score: {accuracy_score(y_test,y_pred)}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through all transformations to determine what's the best in terms of RMSE:\n",
    "\n",
    "rmse = []\n",
    "# raw, normalized and standardized training and testing data\n",
    "trainX = [X_train, X_norm_train, X_stand_train, X_train_pca]\n",
    "testX = [X_test, X_norm_test, X_stand_test, X_test_pca]\n",
    "start_time = time.time()\n",
    "strs=['original', 'normalized', 'standardized', 'pca']\n",
    "# model fitting and measuring RMSE\n",
    "for i in range(len(trainX)):\n",
    "    \n",
    "    # fit\n",
    "    lg1.fit(trainX[i],y_train)\n",
    "    # predict\n",
    "    pred = lg1.predict(testX[i])\n",
    "    # RMSE\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
    "    \n",
    "# visualizing the result\n",
    "df_lg_rmse = pd.DataFrame({'RMSE':rmse},index=['Original','Normalized','Standardized', 'PCA'])\n",
    "df_lg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at a summary of most important predictors or coefficients based on p-value\n",
    "#uses statsmodel library\n",
    "#can change 'X_norm_train' to 'X_stand_train' for diff transforms\n",
    "predictors = ['lapl', 'height', 'width','contours', 'edges', 'std', 'cnt_area', \\\n",
    "            'contrast', 'circularity', 'solidity','complexity','equiv_d','convex_perim',\\\n",
    "            'hull_area', 'perim', 'phi', 'cutoff', 'extreme_points' ,\\\n",
    "             'filled_circular_area_ratio','roundness','perim_area_ratio']\n",
    "\n",
    "y = df_SPHERES['good_bad'] #0 = sphere, 1 = other\n",
    "logit_model=sm.Logit(y_train, X_norm_train, missing='drop')\n",
    "result=logit_model.fit()\n",
    "print(result.summary2(xname=predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw data with no transforms to see distribution shapes\n",
    "df_SPHERES.hist(bins=30,figsize=(15,13));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n",
    "\n",
    "Standardization assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform SIFT (separate ice for training) model\n",
    "separates high and low quality images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SIFT = pd.read_pickle(\"/data/data/saved_models/no_mask/sift_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into x,y\n",
    "x = df_SIFT.drop('good_bad',axis=1)\n",
    "y = df_SIFT['good_bad']\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_SIFT_norm = MinMaxScaler() \n",
    "X_norm_train = scaler_SIFT_norm.fit_transform(X_train)\n",
    "X_norm_test = scaler_SIFT_norm.transform(X_test)\n",
    "scaler_SIFT_stand = StandardScaler() \n",
    "X_stand_train = scaler_SIFT_stand.fit_transform(X_train)\n",
    "X_stand_test = scaler_SIFT_stand.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_stand_train)\n",
    "X_test_pca = pca.transform(X_stand_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "lg_SIFT = LogisticRegression(random_state=13, class_weight='balanced')\n",
    "# fit it\n",
    "lg_SIFT.fit(X_stand_train,y_train)  #don't normalize/standardize y since already between 0-1 for categorical \n",
    "# test\n",
    "y_pred = lg_SIFT.predict(X_stand_test)\n",
    "# performance\n",
    "print(f'Accuracy Score: {accuracy_score(y_test,y_pred)}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine if normalization or standardization is better:\n",
    "\n",
    "rmse = []\n",
    "# raw, normalized and standardized training and testing data\n",
    "trainX = [X_train, X_norm_train, X_stand_train, X_train_pca]\n",
    "testX = [X_test, X_norm_test, X_stand_test, X_test_pca]\n",
    "start_time = time.time()\n",
    "strs=['original', 'normalized', 'standardized', 'pca']\n",
    "# model fitting and measuring RMSE\n",
    "for i in range(len(trainX)):\n",
    "    # fit\n",
    "    lg_SIFT.fit(trainX[i],y_train)\n",
    "    # predict\n",
    "    pred = lg_SIFT.predict(testX[i])\n",
    "    # RMSE\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
    "    \n",
    "# visualizing the result\n",
    "df_lg_rmse = pd.DataFrame({'RMSE':rmse},index=['Original','Normalized','Standardized', 'PCA'])\n",
    "df_lg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'serif',\n",
    "        'size'   : 12}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "labels=['height', 'width', 'laplacian', '# contours', 'edges', '$\\sigma$', 'contour area',\n",
    "       'contrast', 'circularity', 'solidity', 'complexity', 'equiv diameter',\n",
    "       'convex perimeter', 'hull area', 'perimeter', 'aspect ratio', 'cutoff', 'extreme points',\n",
    "       'area ratio', 'roundness', 'perim-area ratio']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(8,7))\n",
    "\n",
    "#SPHERES feature importance\n",
    "importance = lg1.coef_[0]\n",
    "# summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    "#    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "ax1.bar([x for x in range(len(importance))], importance)\n",
    "ax1.set_ylabel('Feature\\nImportance', fontsize=18)\n",
    "ax1.set_title('SPHERES Model', fontsize=20)\n",
    "ax1.set_ylim(-3.2,3.2)\n",
    "\n",
    "#SIFT feature importance\n",
    "importance = lg_SIFT.coef_[0]\n",
    "# summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    "#    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "ax2.bar([x for x in range(len(importance))], importance)\n",
    "#ax2.set_xticks(np.arange(0,21))\n",
    "plt.xticks(np.arange(0,21), labels, rotation=90)\n",
    "ax2.set_ylabel('Feature\\nImportance', fontsize=18)\n",
    "ax2.set_title('SIFT Model', fontsize=20)\n",
    "ax2.set_ylim(-3.2,3.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/data/data/plots/feature_importance_SPHERES_SIFT.pdf', dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['lapl', 'height', 'width','contours', 'edges', 'std', 'cnt_area', \\\n",
    "           'contrast', 'circularity', 'solidity','complexity','equiv_d','convex_perim',\\\n",
    "           'hull_area', 'perim', 'phi', 'cutoff', 'extreme_points' ,\\\n",
    "            'filled_circular_area_ratio','roundness','perim_area_ratio']\n",
    "X = df_SIFT[predictors]\n",
    "X_stand = scaler_SIFT_stand.fit_transform(X)\n",
    "X_norm = scaler_SIFT_norm.fit_transform(X)\n",
    "\n",
    "y = df_SIFT['good_bad'] #0 = sphere, 1 = other\n",
    "logit_model=sm.Logit(y, X_norm, missing='drop')\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=df_SIFT['roundness'], y=df_SIFT['good_bad'], y_jitter=0.03, logistic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=df_SIFT['edges'], y=df_SIFT['good_bad'], y_jitter=0.03, logistic = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through a new data set (not training) and make predictions for SPHERES and SIFT\n",
    "### show images with predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#campaign = '2002_CRYSTAL-FACE-NASA'\n",
    "campaigns=['ARM', 'CRYSTAL_FACE_NASA', 'CRYSTAL_FACE_UND', 'AIRS_II',\n",
    "          'Midcix', 'ICE_L', 'MPACE', 'OLYMPEX']\n",
    "campaigns=['CRYSTAL_FACE_UND']\n",
    "desired_size = 1000\n",
    "spheres_count = 0\n",
    "good_ice_count = 0\n",
    "bad_ice_count = 0\n",
    "cutoff_allowed=10\n",
    "\n",
    "for campaign in campaigns:\n",
    "    print(campaign)\n",
    "    start_time = time.time()\n",
    "    open_dir = '../cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "    #open_dir = 'cpi_data/training_datasets/SPHERES/bad/'\n",
    "    save_dir_good = '../cpi_data/campaigns/'+campaign+'/good_lowcutoff5/'\n",
    "    save_dir_bad = '../cpi_data/campaigns/'+campaign+'/bad/'\n",
    "    for filename in os.listdir(open_dir):\n",
    "\n",
    "            image = Image(open_dir, filename)\n",
    "            image.resize_stretch(desired_size)\n",
    "            image.find_contours()\n",
    "            if len(image.contours)!=0 and image.area() != 0.0:\n",
    "                #image.show_image()\n",
    "                image.morph_contours()\n",
    "                #image.mask_background()            \n",
    "\n",
    "                edges= np.count_nonzero(image.edges())\n",
    "                lapl=image.laplacian()\n",
    "                contours=len(image.contours)\n",
    "                contrast=image.contrast()         \n",
    "\n",
    "                if edges > 0:\n",
    "                    std=np.std(np.nonzero(image.edges()))\n",
    "                else:\n",
    "                    std=0\n",
    "                height=image.height_og\n",
    "                width=image.width_og\n",
    "                cnt_area=image.area()\n",
    "                solidity=image.solidity()\n",
    "                complexity=image.complexity()\n",
    "                equiv_d=image.equiv_d()\n",
    "                convex_perim=image.convex_perim(True)\n",
    "                hull_area=image.hull_area()\n",
    "                perim=image.perim()\n",
    "                phi=image.phi()\n",
    "                circularity=image.circularity()\n",
    "                cutoff=image.cutoff()\n",
    "                perim_area_ratio=image.perim_area_ratio()\n",
    "                roundness=image.roundness()\n",
    "                filled_circular_area_ratio=image.filled_circular_area_ratio()\n",
    "                extreme_points=image.extreme_points()\n",
    "            else:\n",
    "                height=0\n",
    "                width=0\n",
    "                cnt_area=0\n",
    "                solidity=0\n",
    "                complexity=0\n",
    "                equiv_d=0\n",
    "                convex_perim=0\n",
    "                hull_area=0\n",
    "                perim=0\n",
    "                phi=0\n",
    "                circularity=0\n",
    "                cutoff=0\n",
    "                perim_area_ratio=0\n",
    "                roundness=0\n",
    "                filled_circular_area_ratio=0\n",
    "                extreme_points=0\n",
    "                std=0\n",
    "                edges=0\n",
    "                lapl=0\n",
    "                contours=0\n",
    "                contrast=0\n",
    "\n",
    "            #loop through all at once and append predictors \n",
    "            #for each row if spheres, also predict good \n",
    "            dicts = {}\n",
    "            keys = ['lapl', 'height', 'width','contours', 'edges', 'std', 'cnt_area', \\\n",
    "               'contrast', 'circularity', 'solidity','complexity','equiv_d','convex_perim',\\\n",
    "               'hull_area', 'perim', 'phi', 'cutoff', 'extreme_points' ,\\\n",
    "                'filled_circular_area_ratio','roundness','perim_area_ratio']\n",
    "\n",
    "            values =  [lapl, height, width, contours, edges, std, cnt_area, \\\n",
    "                   contrast, circularity, solidity, complexity, equiv_d, convex_perim,\\\n",
    "                   hull_area, perim, phi, cutoff, extreme_points, filled_circular_area_ratio,\\\n",
    "                      roundness, perim_area_ratio]\n",
    "\n",
    "            for key, val in zip(keys, values):\n",
    "                dicts[key] = val\n",
    "            df_pred = pd.DataFrame(dicts, index=[0])\n",
    "            #Regression model prediction\n",
    "            pred_SPHERES = scaler_SPHERES_stand.transform(df_pred)\n",
    "            pred_SPHERES = lg1.predict(pred_SPHERES)\n",
    "\n",
    "            #SVC prediction\n",
    "            #pred = clf_rbf.predict(df)\n",
    "            #pred = clf_linear.predict(df)\n",
    "            #pred = clf_sig.predict(df)\n",
    "\n",
    "            #PCA prediction\n",
    "            #pred = clf_rfc.predict(df)\n",
    "\n",
    "            if pred_SPHERES[0] < 0.25:\n",
    "                if len(image.contours) != 0 and image.cutoff() < cutoff_allowed:\n",
    "                    spheres_count+=1\n",
    "                    #print(image.cutoff())\n",
    "                    #print(\"GOOD sphere\")\n",
    "                    #image.show_image()\n",
    "                    plt.show()\n",
    "                    #image.save_image(save_dir_good)             \n",
    "\n",
    "            else:\n",
    "                #print(image.cutoff())\n",
    "    #             print(\"BAD sphere\")\n",
    "    #             image.show_image()\n",
    "    #             plt.show()\n",
    "                #image.save_image(save_dir_bad)\n",
    "\n",
    "                #If non sphere, find ice that is not blurry or broken\n",
    "                pred_SIFT = scaler_SIFT_stand.transform(df_pred)\n",
    "                pred_SIFT = lg_SIFT.predict(pred_SIFT)\n",
    "                if pred_SIFT[0] < 0.25 and image.cutoff() < cutoff_allowed:\n",
    "                    #print(\"GOOD ice\")\n",
    "                    image.show_image()\n",
    "                    plt.show()\n",
    "                    good_ice_count +=1\n",
    "                    #image.save_image(save_dir_good)\n",
    "\n",
    "                else:\n",
    "                    #print('BAD ice')\n",
    "                    #image.show_image()\n",
    "                    plt.show()\n",
    "                    bad_ice_count +=1\n",
    "    end_time = time.time()\n",
    "    print(campaign, spheres_count, good_ice_count, bad_ice_count, end_time-start_time)\n",
    "    f.write(campaign, spheres_count, good_ice_count, bad_ice_count, end_time-start_time, '\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the above in parallel (was saving only good images of ice)\n",
    "campaigns = ['MPACE']\n",
    "for campaign in campaigns:\n",
    "    print(campaign)\n",
    "    time_start = time.time()\n",
    "    open_dir = 'cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "    save_dir_good = 'cpi_data/campaigns/'+campaign+'/good_lowcutoff_timing/'\n",
    "    save_dir_bad = 'cpi_data/campaigns/'+campaign+'/bad/'\n",
    "    desired_size = 1000\n",
    "\n",
    "    iterable = [file for file in os.listdir(open_dir)]\n",
    "    p = Pool(multiprocessing.cpu_count() - 1)\n",
    "    p.map(make_new_prediction, iterable)\n",
    "    print(time.time() - time_start)\n",
    "    p.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
