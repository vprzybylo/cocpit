{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"6tGmiuOfY08czs2b4SHaHI2hw\",\n",
    "    project_name=\"multi-campaigns\",\n",
    "    workspace=\"vprzybylo\",\n",
    ")\n",
    "experiment.log_code(\"/data/data/notebooks/Pytorch_mult_campaigns.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from natsort import natsorted\n",
    "from PIL import Image, ImageFile\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {\n",
    "    \"axes.labelsize\": \"xx-large\",\n",
    "    \"axes.titlesize\": \"xx-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"xx-large\",\n",
    "}\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = original_tuple + (path,)\n",
    "        return (tuple_with_path, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal pull from classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(train_imgs, nclasses):\n",
    "    # only weight the training dataset\n",
    "\n",
    "    class_sample_counts = [0] * nclasses\n",
    "    for item in train_imgs:\n",
    "        class_sample_counts[item[1]] += 1\n",
    "    print(\"counts per class: \", class_sample_counts)\n",
    "\n",
    "    #     weight_per_class = [0.] * nclasses\n",
    "    #     N = float(sum(class_sample_counts))\n",
    "    #     for i in range(nclasses):\n",
    "    #         weight_per_class[i] = N/float(class_sample_counts[i])\n",
    "    #     weight = [0] * len(images)\n",
    "    #     for idx, val in enumerate(images):\n",
    "    #         weight[idx] = weight_per_class[val[1]]\n",
    "\n",
    "    class_weights = 1.0 / torch.Tensor(class_sample_counts)\n",
    "    train_targets = [sample[1] for sample in train_imgs]\n",
    "    train_samples_weights = [class_weights[class_id] for class_id in train_targets]\n",
    "\n",
    "    return class_sample_counts, torch.DoubleTensor(train_samples_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram_classcounts(class_names, class_counts):\n",
    "    fig, ax = plt.subplots(figsize=(9.5, 5))\n",
    "\n",
    "    width = 0.75  # the width of the bars\n",
    "    ind = np.arange(len(class_counts))  # the x locations for the groups\n",
    "    ax.barh(\n",
    "        class_names,\n",
    "        class_counts,\n",
    "        width,\n",
    "        color=\"blue\",\n",
    "        align=\"center\",\n",
    "        tick_label=class_names,\n",
    "    )\n",
    "    # ax.set_yticks(ind+width/2)\n",
    "    # plt.xticks(rotation=-90, ha='center')\n",
    "\n",
    "    for i, v in enumerate(class_counts):\n",
    "        ax.text(v, i - 0.1, str(v), color=\"blue\")\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    # ax.set_xlim(0,2500)\n",
    "    plt.savefig(\"../plots/class_counts.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_val(\n",
    "    save_model,\n",
    "    val_loader_savename,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    class_names,\n",
    "    datadir,\n",
    "    batch_size,\n",
    "    show_sample=True,\n",
    "    num_workers=32,\n",
    "    valid_size=0.8,\n",
    "):\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # For an unbalanced dataset we create a weighted sampler\n",
    "    class_counts, train_samples_weights = make_weights_for_balanced_classes(\n",
    "        train_data.dataset.imgs, len(range(num_classes))\n",
    "    )\n",
    "    make_histogram_classcounts(class_names, class_counts)\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "        train_samples_weights, len(train_samples_weights), replacement=True\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_data.dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_sampler = SubsetRandomSampler(val_data.indices)\n",
    "    # print(train_data.indices, val_data.indices)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_data.dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    if save_model:\n",
    "        torch.save(valloader, \"/data/data/saved_models/\" + masked_dir + \"val_loader.pt\")\n",
    "\n",
    "    #     val_samples_weights = make_weights_for_balanced_classes(val_data.dataset.imgs, len(range(num_classes)))\n",
    "\n",
    "    #     val_sampler = torch.utils.data.sampler.WeightedRandomSampler(val_samples_weights,\n",
    "    #                                                                    len(val_samples_weights),\n",
    "    #                                                                    replacement=True)\n",
    "    #     valloader = torch.utils.data.DataLoader(val_data.dataset, batch_size=batch_size,\n",
    "    #                                             sampler = val_sampler, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    if show_sample:\n",
    "        show_sample(train_data, train_sampler)\n",
    "    print(len(trainloader), len(valloader))\n",
    "\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(train_data, train_sampler):\n",
    "\n",
    "    batch_size_sampler = 20\n",
    "    sample_loader = torch.utils.data.DataLoader(\n",
    "        train_data.dataset,\n",
    "        batch_size=batch_size_sampler,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    data_iter = iter(sample_loader)\n",
    "\n",
    "    images, labels, paths = data_iter.next()\n",
    "    fig, ax = plt.subplots(batch_size_sampler // 5, 5, figsize=(10, 8))\n",
    "\n",
    "    for j in range(images.size()[0]):\n",
    "\n",
    "        # Undo preprocessing\n",
    "        image = images[j].permute(1, 2, 0).cpu().numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "        image = std * image + mean\n",
    "\n",
    "        # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "        image = np.clip(image, 0, 1)\n",
    "        ax = ax.flatten()\n",
    "        ax[j].set_title(str(class_names[labels[j]]))\n",
    "        ax[j].axis(\"off\")\n",
    "        ax[j].imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(datadir, batch_size, num_workers, shuffle=True, pin_memory=True):\n",
    "    \"\"\"\n",
    "    Utility function for loading and returning a multi-process\n",
    "    test iterator\n",
    "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - shuffle: whether to shuffle the dataset after every epoch.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - data_loader: test set iterator.\n",
    "    \"\"\"\n",
    "    transforms_ = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),  # resizing helps memory usage\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    all_data_wpath = ImageFolderWithPaths(datadir, transform=transforms_)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        all_data_wpath,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "def set_parameter_requires_grad(model, feature_extract):\n",
    "    if feature_extract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(\n",
    "    model_name, num_classes, feature_extract=False, use_pretrained=False\n",
    "):\n",
    "    # all input size of 224\n",
    "    if model_name == \"resnet18\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"resnet152\":\n",
    "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"vgg16\":\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"vgg19\":\n",
    "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_1(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(\n",
    "            512, num_classes, kernel_size=(7, 7), stride=(2, 2)\n",
    "        )\n",
    "        # model_ft.num_classes = num_classes\n",
    "\n",
    "    elif model_name == \"densenet169\":\n",
    "        model_ft = models.densenet169(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"densenet201\":\n",
    "        model_ft = models.densenet201(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"efficient\":\n",
    "        # torch.hub.list('rwightman/gen-efficientnet-pytorch')\n",
    "        # model_ft = torch.hub.load('rwightman/gen-efficientnet-pytorch', 'efficientnet_b0', pretrained=False)\n",
    "        model_ft = EfficientNet.from_name(\"efficientnet-b0\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dropout(model, drop_rate=0.1):\n",
    "    for name, child in model.named_children():\n",
    "        if isinstance(child, torch.nn.Dropout):\n",
    "            child.p = drop_rate\n",
    "        set_dropout(child, drop_rate=drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    kfold,\n",
    "    model_name,\n",
    "    model_savename,\n",
    "    acc_savename_train,\n",
    "    acc_savename_val,\n",
    "    save_acc,\n",
    "    save_model,\n",
    "    dataloaders_dict,\n",
    "    epochs,\n",
    "    num_classes,\n",
    "    feature_extract=False,\n",
    "):\n",
    "    # feature extract False for all layers to be updated\n",
    "\n",
    "    set_dropout(model, drop_rate=0.0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Send the model to GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model.parameters()\n",
    "    # print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                # print(\"\\t\",name)\n",
    "    # else:\n",
    "    # for name,param in model.named_parameters():\n",
    "    # if param.requires_grad == True:\n",
    "    # print(\"\\t\",name)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    # step_size: at how many multiples of epoch you decay\n",
    "    # step_size = 1, after every 1 epoch, new_lr = lr*gamma\n",
    "    # step_size = 2, after every 2 epoch, new_lr = lr*gamma\n",
    "    # gamma = decaying factor\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=0, verbose=True, eps=1e-04\n",
    "    )\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc_val = 0.0\n",
    "    since_total = time.time()\n",
    "\n",
    "    step = 0\n",
    "    label_counts = [0] * len(range(num_classes))\n",
    "    for epoch in range(epochs):\n",
    "        since_epoch = time.time()\n",
    "        # print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            print(\"Phase: {}\".format(phase))\n",
    "            totals_train = 0\n",
    "            totals_val = 0\n",
    "            running_loss_train = 0.0\n",
    "            running_loss_val = 0.0\n",
    "            running_corrects_train = 0\n",
    "            running_corrects_val = 0\n",
    "\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                # logger = logger_train\n",
    "\n",
    "            else:\n",
    "                model.eval()\n",
    "                # logger = logger_val\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels, paths) in enumerate(dataloaders_dict[phase]):\n",
    "                print(i)\n",
    "                for n in range(len(range(num_classes))):\n",
    "                    label_counts[n] += len(np.where(labels.numpy() == n)[0])\n",
    "\n",
    "                #                 for n in range(len(range(num_classes))):\n",
    "                #                     print(\"batch index {}, {} counts: {}\".format(\n",
    "                #                         i, n, (labels == n).sum()))\n",
    "\n",
    "                #                print('LABEL COUNT = ', label_counts)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # print(inputs.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()  # a clean up step for PyTorch\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()  # compute updates for each parameter\n",
    "                        optimizer.step()  # make the updates for each parameter\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    # Batch accuracy and loss statistics\n",
    "                    batch_loss_train = loss.item() * inputs.size(0)\n",
    "                    batch_corrects_train = torch.sum(preds == labels.data)\n",
    "                    # tensorboard_logging(logger, batch_loss_train, labels, batch_corrects_train, step, model)\n",
    "\n",
    "                    # for accuracy and loss statistics overall\n",
    "                    running_loss_train += loss.item() * inputs.size(0)\n",
    "                    running_corrects_train += torch.sum(preds == labels.data)\n",
    "                    totals_train += labels.size(0)\n",
    "\n",
    "                    if (i + 1) % 5 == 0:\n",
    "                        print(\n",
    "                            \"Training, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "                                i + 1,\n",
    "                                len(dataloaders_dict[phase]),\n",
    "                                batch_loss_train / labels.size(0),\n",
    "                                float(batch_corrects_train) / labels.size(0),\n",
    "                            )\n",
    "                        )\n",
    "                    step += 1\n",
    "\n",
    "                else:\n",
    "                    # Batch accuracy and loss statistics\n",
    "                    batch_loss_val = loss.item() * inputs.size(0)\n",
    "                    batch_corrects_val = torch.sum(preds == labels.data)\n",
    "\n",
    "                    # for accuracy and loss statistics overall\n",
    "                    running_loss_val += loss.item() * inputs.size(0)\n",
    "                    running_corrects_val += torch.sum(preds == labels.data)\n",
    "                    totals_val += labels.size(0)\n",
    "\n",
    "                    if (i + 1) % 5 == 0:\n",
    "                        print(\n",
    "                            \"Validation, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "                                i + 1,\n",
    "                                len(dataloaders_dict[phase]),\n",
    "                                batch_loss_val / labels.size(0),\n",
    "                                float(batch_corrects_val) / labels.size(0),\n",
    "                            )\n",
    "                        )\n",
    "            if phase == \"train\":\n",
    "                # epoch loss and accuracy stats\n",
    "                epoch_loss_train = running_loss_train / totals_train\n",
    "                epoch_acc_train = running_corrects_train.double() / totals_train\n",
    "                scheduler.step(\n",
    "                    epoch_acc_train\n",
    "                )  # reduce learning rate if not improving acc\n",
    "                # experiment.log_metric('train scheduler', scheduler)\n",
    "\n",
    "                # write acc and loss to file within epoch iteration\n",
    "                if save_acc:\n",
    "                    with open(acc_savename_train, \"a\", newline=\"\") as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerow(\n",
    "                            [\n",
    "                                model_name,\n",
    "                                epoch,\n",
    "                                kfold,\n",
    "                                epoch_acc_train.cpu().numpy(),\n",
    "                                epoch_loss_train,\n",
    "                            ]\n",
    "                        )\n",
    "                    file.close()\n",
    "\n",
    "                print(\n",
    "                    \"Training Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(\n",
    "                        epoch + 1, epochs, epoch_loss_train, epoch_acc_train\n",
    "                    )\n",
    "                )\n",
    "                train_acc_history.append(epoch_acc_train)\n",
    "                train_loss_history.append(epoch_loss_train)\n",
    "                # experiment.log_metric('epoch_acc_train', epoch_acc_train*100)\n",
    "                # experiment.log_metric('epoch_loss_train', epoch_loss_train)\n",
    "\n",
    "            else:\n",
    "                epoch_loss_val = running_loss_val / totals_val\n",
    "                epoch_acc_val = running_corrects_val.double() / totals_val\n",
    "                scheduler.step(\n",
    "                    epoch_acc_val\n",
    "                )  # reduce learning rate if not improving acc\n",
    "                # experiment.log_metric('val scheduler', scheduler)\n",
    "\n",
    "                # write acc and loss to file within epoch iteration\n",
    "                if save_acc:\n",
    "                    with open(acc_savename_val, \"a\", newline=\"\") as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerow(\n",
    "                            [\n",
    "                                model_name,\n",
    "                                epoch,\n",
    "                                kfold,\n",
    "                                epoch_acc_val.cpu().numpy(),\n",
    "                                epoch_loss_val,\n",
    "                            ]\n",
    "                        )\n",
    "                    file.close()\n",
    "\n",
    "                print(\n",
    "                    \"Validation Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(\n",
    "                        epoch + 1, epochs, epoch_loss_val, epoch_acc_val\n",
    "                    )\n",
    "                )\n",
    "                val_acc_history.append(epoch_acc_val)\n",
    "                val_loss_history.append(epoch_loss_val)\n",
    "                # experiment.log_metric('epoch_acc_val', epoch_acc_val*100)\n",
    "                # experiment.log_metric('epoch_loss_val', epoch_loss_val)\n",
    "\n",
    "                # deep copy the model\n",
    "                if epoch_acc_val > best_acc_val:\n",
    "                    best_acc_val = epoch_acc_val\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    # save/load best model weights\n",
    "                    if save_model:\n",
    "                        torch.save(model, model_savename + \"_\" + model_name)\n",
    "\n",
    "        time_elapsed = time.time() - since_epoch\n",
    "        print(\n",
    "            \"Epoch complete in {:.0f}m {:.0f}s\".format(\n",
    "                time_elapsed // 60, time_elapsed % 60\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_elapsed = time.time() - since_total\n",
    "    print(\n",
    "        \"All epochs comlete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # with open('/data/data/saved_models/model_timing.csv', 'a', newline='') as file:\n",
    "    #    writer = csv.writer(file)\n",
    "    #    writer.writerow([model_name, epoch, kfold, time_elapsed])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    all_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    # custom dataset that includes entire path\n",
    "    all_data_wpath = ImageFolderWithPaths(params[\"data_dir\"], transform=all_transforms)\n",
    "\n",
    "    for batch_size in params[\"batch_size\"]:\n",
    "        print(\"BATCH SIZE: \", batch_size)\n",
    "        for model_name in params[\"model_names\"]:\n",
    "            print(\"MODEL: \", model_name)\n",
    "            for epochs in params[\"max_epochs\"]:\n",
    "                print(\"MAX EPOCH: \", epochs)\n",
    "\n",
    "                # K-FOLD\n",
    "                if params[\"kfold\"] != 0:\n",
    "                    train_score = pd.Series(dtype=np.float64)\n",
    "                    val_score = pd.Series(dtype=np.float64)\n",
    "\n",
    "                    total_size = len(all_data_wpath)\n",
    "                    fraction = 1 / params[\"kfold\"]\n",
    "                    seg = int(total_size * fraction)\n",
    "                    # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n",
    "                    # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
    "\n",
    "                    for i in range(params[\"kfold\"]):\n",
    "                        print(\"KFOLD: \", i)\n",
    "                        trll = 0\n",
    "                        trlr = i * seg\n",
    "                        vall = trlr\n",
    "                        valr = i * seg + seg\n",
    "                        trrl = valr\n",
    "                        trrr = total_size\n",
    "\n",
    "                        print(\n",
    "                            \"train indices: [%d,%d),[%d,%d), test indices: [%d,%d)\"\n",
    "                            % (trll, trlr, trrl, trrr, vall, valr)\n",
    "                        )\n",
    "\n",
    "                        train_left_indices = list(range(trll, trlr))\n",
    "                        train_right_indices = list(range(trrl, trrr))\n",
    "\n",
    "                        train_indices = train_left_indices + train_right_indices\n",
    "                        val_indices = list(range(vall, valr))\n",
    "\n",
    "                        train_data = torch.utils.data.dataset.Subset(\n",
    "                            all_data_wpath, train_indices\n",
    "                        )\n",
    "                        val_data = torch.utils.data.dataset.Subset(\n",
    "                            all_data_wpath, val_indices\n",
    "                        )\n",
    "\n",
    "                        train_loader, val_loader = load_split_train_val(\n",
    "                            save_model,\n",
    "                            val_loader_savename,\n",
    "                            train_data,\n",
    "                            val_data,\n",
    "                            class_names=params[\"class_names\"],\n",
    "                            datadir=params[\"data_dir\"],\n",
    "                            batch_size=batch_size,\n",
    "                            show_sample=False,\n",
    "                            num_workers=num_workers,\n",
    "                        )\n",
    "\n",
    "                        dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "                        # INITIALIZE MODEL\n",
    "                        model = initialize_model(model_name, num_classes)\n",
    "\n",
    "                        # TRAIN MODEL\n",
    "                        train_model(\n",
    "                            model,\n",
    "                            i,\n",
    "                            model_name,\n",
    "                            model_savename,\n",
    "                            acc_savename_train,\n",
    "                            acc_savename_val,\n",
    "                            save_acc,\n",
    "                            save_model,\n",
    "                            dataloaders_dict,\n",
    "                            epochs,\n",
    "                            num_classes,\n",
    "                        )\n",
    "                else:  # no kfold\n",
    "                    i = 0\n",
    "                    train_length = int(valid_size * len(all_data_wpath))\n",
    "                    val_length = len(all_data_wpath) - train_length\n",
    "                    train_data, val_data = torch.utils.data.random_split(\n",
    "                        all_data_wpath, (train_length, val_length)\n",
    "                    )\n",
    "\n",
    "                    train_loader, val_loader = load_split_train_val(\n",
    "                        save_model,\n",
    "                        val_loader_savename,\n",
    "                        train_data,\n",
    "                        val_data,\n",
    "                        class_names=params[\"class_names\"],\n",
    "                        datadir=params[\"data_dir\"],\n",
    "                        batch_size=batch_size,\n",
    "                        show_sample=False,\n",
    "                        num_workers=num_workers,\n",
    "                    )\n",
    "\n",
    "                    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "                    # INITIALIZE MODEL\n",
    "                    model = initialize_model(model_name, num_classes)\n",
    "\n",
    "                    # TRAIN MODEL\n",
    "                    train_model(\n",
    "                        model,\n",
    "                        i,\n",
    "                        model_name,\n",
    "                        model_savename,\n",
    "                        acc_savename_train,\n",
    "                        acc_savename_val,\n",
    "                        save_acc,\n",
    "                        save_model,\n",
    "                        dataloaders_dict,\n",
    "                        epochs,\n",
    "                        num_classes,\n",
    "                    )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    params = {\n",
    "        \"kfold\": 0,  # set to 0 to turn off kfold cross validation\n",
    "        \"batch_size\": [128],\n",
    "        \"masked\": False,\n",
    "        \"max_epochs\": [50],\n",
    "        \"data_dir\": \"/data/data/cpi_data/training_datasets/hand_labeled_resized_multcampaigns_v1.0.0_no_blank/\",\n",
    "        \"class_names\": [\n",
    "            \"aggregates\",\n",
    "            \"budding\",\n",
    "            \"bullets\",\n",
    "            \"columns\",\n",
    "            \"compact irregulars\",\n",
    "            \"fragments/blurry\",\n",
    "            \"plates\",\n",
    "            \"rimed\",\n",
    "            \"spheres\",\n",
    "        ],\n",
    "        \"model_names\": [\n",
    "            \"resnet18\",\n",
    "            \"resnet34\",\n",
    "            \"resnet152\",\n",
    "            \"alexnet\",\n",
    "            \"vgg16\",\n",
    "            \"vgg19\",\n",
    "            \"densenet169\",\n",
    "            \"densenet201\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    if params[\"masked\"]:\n",
    "        masked_dir = \"masked/\"\n",
    "    else:\n",
    "        masked_dir = \"no_mask/\"\n",
    "\n",
    "    model_savename = (\n",
    "        \"/data/data/saved_models/\"\n",
    "        + masked_dir\n",
    "        + \"e\"\n",
    "        + str(params[\"max_epochs\"][0])\n",
    "        + \"_bs\"\n",
    "        + str(params[\"batch_size\"][0])\n",
    "        + \"_k\"\n",
    "        + str(params[\"kfold\"])\n",
    "        + \"_\"\n",
    "        + str(len(params[\"model_names\"]))\n",
    "        + \"models\"\n",
    "    )\n",
    "    acc_savename_train = (\n",
    "        \"/data/data/saved_models/\"\n",
    "        + masked_dir\n",
    "        + \"save_train_acc_loss_e\"\n",
    "        + str(params[\"max_epochs\"][0])\n",
    "        + \"_bs\"\n",
    "        + str(params[\"batch_size\"][0])\n",
    "        + \"_k\"\n",
    "        + str(params[\"kfold\"])\n",
    "        + \"_\"\n",
    "        + str(len(params[\"model_names\"]))\n",
    "        + \"models.csv\"\n",
    "    )\n",
    "    acc_savename_val = (\n",
    "        \"/data/data/saved_models/\"\n",
    "        + masked_dir\n",
    "        + \"save_val_acc_loss_e\"\n",
    "        + str(params[\"max_epochs\"][0])\n",
    "        + \"_bs\"\n",
    "        + str(params[\"batch_size\"][0])\n",
    "        + \"_k\"\n",
    "        + str(params[\"kfold\"])\n",
    "        + \"_\"\n",
    "        + str(len(params[\"model_names\"]))\n",
    "        + \"models.csv\"\n",
    "    )\n",
    "    val_loader_savename = (\n",
    "        \"/data/data/saved_models/\"\n",
    "        + masked_dir\n",
    "        + \"val_loader_e\"\n",
    "        + str(params[\"max_epochs\"][0])\n",
    "        + \"_bs\"\n",
    "        + str(params[\"batch_size\"][0])\n",
    "        + \"_k\"\n",
    "        + str(params[\"kfold\"])\n",
    "        + \"_\"\n",
    "        + str(len(params[\"model_names\"]))\n",
    "        + \"models.csv\"\n",
    "    )\n",
    "    save_acc = False\n",
    "    save_model = False\n",
    "    valid_size = 0.8  # 80-20 split training-val\n",
    "    num_workers = 20  # change to # of cores available to load images\n",
    "    num_classes = len(params[\"class_names\"])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY PLOT for training and validation\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    np.arange(1, (params[\"max_epochs\"][0] + 1)),\n",
    "    [i.cpu().numpy() * 100 for i in model_train_accs[0]],\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, (params[\"max_epochs\"][0] + 1)),\n",
    "    [i.cpu().numpy() * 100 for i in model_val_accs[0]],\n",
    "    label=\"validation\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params[\"max_epochs\"][0] + 1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "# LOSS PLOT for training and validation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    np.arange(1, (params[\"max_epochs\"][0] + 1)),\n",
    "    [i for i in model_train_loss[0]],\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, (params[\"max_epochs\"][0] + 1)),\n",
    "    [i for i in model_val_loss[0]],\n",
    "    label=\"validation\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params[\"max_epochs\"][0] + 1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
