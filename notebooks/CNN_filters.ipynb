{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBSztV_ob01R"
   },
   "source": [
    "# Visualization of CNN Layers and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The saliency map will show the strength for each pixel contribution to the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {\n",
    "    \"axes.labelsize\": \"large\",\n",
    "    \"axes.titlesize\": \"large\",\n",
    "    \"xtick.labelsize\": \"large\",\n",
    "    \"ytick.labelsize\": \"large\",\n",
    "}\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, size=224):\n",
    "    # Preprocess the image\n",
    "    # convert to tensor\n",
    "    # normalize\n",
    "    # and convert to correct shape\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize((size, size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    tensor.requires_grad = True\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def get_saliency(image, model):\n",
    "\n",
    "    # run the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # find the gradient with respect to the input image; call requires_grad_ on it\n",
    "    image.requires_grad_()\n",
    "\n",
    "    \"\"\"\n",
    "    forward pass through the model to get the scores\n",
    "    note that VGG-19 model doesn't perform softmax at the end\n",
    "    we also don't need softmax, just need scores\n",
    "    \"\"\"\n",
    "\n",
    "    scores = model(image)\n",
    "\n",
    "    # Get the index corresponding to the maximum score and the maximum score itself.\n",
    "    score_max_index = scores.argmax()\n",
    "    score_max = scores[0, score_max_index]\n",
    "\n",
    "    \"\"\"\n",
    "    backward function on score_max performs the backward pass in the computation graph and calculates the gradient of \n",
    "    score_max with respect to nodes in the computation graph\n",
    "    \"\"\"\n",
    "    score_max.backward()\n",
    "\n",
    "    \"\"\"\n",
    "    Saliency would be the gradient with respect to the input image now. But note that the input image has 3 channels,\n",
    "    R, G and B. To derive a single class saliency value for each pixel (i, j),  we take the maximum magnitude\n",
    "    across all colour channels.\n",
    "    \"\"\"\n",
    "    saliency, _ = torch.max(image.grad.data.abs(), dim=1)\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    \"/data/data/saved_models/no_mask/e20_bs64_1models_vgg_16_v1.3.0_vgg16\"\n",
    ")\n",
    "class_names = {\n",
    "    \"aggs\": \"Aggregate\",\n",
    "    \"budding\": \"Budding\",\n",
    "    \"bullets\": \"Bullet\",\n",
    "    \"columns\": \"Column\",\n",
    "    \"compact_irregs\": \"Compact\",\n",
    "    \"fragments\": \"Fragment\",\n",
    "    \"plates\": \"Plate\",\n",
    "    \"rimed_aggs\": \"Rimed\",\n",
    "    \"spheres\": \"Sphere\",\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(6, 3, figsize=(5, 13))\n",
    "for x, class_ in enumerate(class_names.items()):\n",
    "    open_dir = (\n",
    "        \"/data/data/cpi_data/training_datasets/hand_labeled_resized_v1.3.0_no_blank/%s/\"\n",
    "        % class_[0]\n",
    "    )\n",
    "    y = 0\n",
    "    y1 = 1\n",
    "    if x >= 3 and x<6:\n",
    "        x -= 3\n",
    "        y = 2\n",
    "        y1 = 3\n",
    "    if x >= 6 and x<9:\n",
    "        x -=6\n",
    "        y = 4\n",
    "        y1 = 5\n",
    "    if x >= 9:\n",
    "        x -=9\n",
    "    file = os.listdir(open_dir)[21]\n",
    "    image = Image.open(open_dir + file).convert(\"RGB\")\n",
    "    #print(y,x)\n",
    "    ax[y, x].imshow(image)\n",
    "    ax[y, x].set_title(class_[1])\n",
    "    ax[y, 0].set_ylabel(\"Original Image\")\n",
    "    ax[y, x].axes.xaxis.set_ticks([])\n",
    "    ax[y, x].axes.yaxis.set_ticks([])\n",
    "    \n",
    "    \n",
    "    image = preprocess(image)\n",
    "    saliency = get_saliency(image, model)\n",
    "\n",
    "    # code to plot the saliency map as a heatmap\n",
    "    ax[y1, x].imshow(saliency[0], cmap=plt.cm.hot)\n",
    "    \n",
    "    ax[y1, 0].set_ylabel(\"Saliency Map\")\n",
    "    ax[y1, x].axes.xaxis.set_ticks([])\n",
    "    ax[y1, x].axes.yaxis.set_ticks([])\n",
    "fig.savefig('/data/data/plots/saliency_maps.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNNVisualisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
